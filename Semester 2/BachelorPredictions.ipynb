{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711294dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2883fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Season 1...\n",
      "Scraping Season 2...\n",
      "Scraping Season 3...\n",
      "Scraping Season 4...\n",
      "Scraping Season 5...\n",
      "Scraping Season 6...\n",
      "Scraping Season 7...\n",
      "Scraping Season 8...\n",
      "Scraping Season 9...\n",
      "Scraping Season 10...\n",
      "Scraping Season 11...\n",
      "Scraping Season 12...\n",
      "Scraping Season 13...\n",
      "Scraping Season 14...\n",
      "Scraping Season 15...\n",
      "Scraping Season 16...\n",
      "Scraping Season 17...\n",
      "Scraping Season 18...\n",
      "Scraping Season 19...\n",
      "Scraping Season 20...\n",
      "Scraping Season 21...\n",
      "Scraping Season 22...\n",
      "Scraping Season 23...\n",
      "Scraping Season 24...\n",
      "Scraping Season 25...\n",
      "Scraping Season 26...\n",
      "Scraping Season 27...\n",
      "Scraping Season 28...\n",
      "Scraping Season 29...\n",
      "Contestants data saved to 'bachelor_fandom_contestants.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time  # Import for delay\n",
    "\n",
    "# Define the base URL of the Bachelor Nation Fandom Wiki\n",
    "BASE_URL = \"https://bachelor-nation.fandom.com/wiki\"\n",
    "\n",
    "def get_contestants_from_season(season_url, season_number):\n",
    "    \"\"\"Scrapes contestant data from a single season page.\"\"\"\n",
    "    contestants = []\n",
    "    response = requests.get(season_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data for Season {season_number}\")\n",
    "        return contestants\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Locate contestant table or list (modify based on structure)\n",
    "    contestant_rows = soup.select('table.wikitable tr')  # Example for wikitable format\n",
    "\n",
    "    for row in contestant_rows[1:]:  # Skip the header row\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) < 2:\n",
    "            continue  # Skip malformed rows\n",
    "\n",
    "        name = cells[0].text.strip()  # Contestant name\n",
    "        notable_info = cells[1].text.strip() if len(cells) > 1 else \"N/A\"  # Notable information\n",
    "        \n",
    "        contestants.append({\n",
    "            \"Contestant Name\": name,\n",
    "            \"Season\": season_number,\n",
    "            \"Notable Information\": notable_info\n",
    "        })\n",
    "    \n",
    "    return contestants\n",
    "\n",
    "def scrape_bachelor_fandom():\n",
    "    \"\"\"Scrapes all contestants from the Bachelor Nation Fandom Wiki.\"\"\"\n",
    "    all_contestants = []\n",
    "    \n",
    "    for season_number in range(1, 30):  # Adjust range for total seasons\n",
    "        season_url = f\"{BASE_URL}/The_Bachelor_(Season_{season_number})\"\n",
    "        print(f\"Scraping Season {season_number}...\")\n",
    "        season_contestants = get_contestants_from_season(season_url, season_number)\n",
    "        all_contestants.extend(season_contestants)\n",
    "\n",
    "        # Add a delay to prevent rate limiting\n",
    "        time.sleep(3)  # Adjust delay as necessary (e.g., 3 seconds)\n",
    "\n",
    "    return all_contestants\n",
    "\n",
    "# Run the scraper\n",
    "contestants_data = scrape_bachelor_fandom()\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "df = pd.DataFrame(contestants_data)\n",
    "df.to_csv(\"bachelor_fandom_contestants.csv\", index=False)\n",
    "print(\"Contestants data saved to 'bachelor_fandom_contestants.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293ae58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
